<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>cards/math/jaccard-similarity.md on Datumorphism</title><link>/links/cards/math/jaccard-similarity.md/</link><description>Recent content in cards/math/jaccard-similarity.md on Datumorphism</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><atom:link href="/links/cards/math/jaccard-similarity.md/index.xml" rel="self" type="application/rss+xml"/><item><title>Cosine Similarity</title><link>/cards/math/cosine-similarity/</link><pubDate>Mon, 06 May 2019 00:00:00 +0000</pubDate><guid>/cards/math/cosine-similarity/</guid><description>As simple as the inner product of two vectors
$$ d_{cos} = \frac{\vec A}{\vert \vec A \vert} \cdot \frac{\vec B }{ \vert \vec B \vert} $$
Examples To use cosine similarity, we have to vectorize the words first. There are many different methods to achieve this. For the purpose of illustrating cosine similarity, we use term frequency.
Term frequency is the occurrence of the words. We do not deal with duplications so duplicate words will have some effect on the similarity.</description></item></channel></rss>