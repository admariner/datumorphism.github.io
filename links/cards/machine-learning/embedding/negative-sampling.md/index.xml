<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>cards/machine-learning/embedding/negative-sampling.md on Datumorphism</title><link>/links/cards/machine-learning/embedding/negative-sampling.md/</link><description>Recent content in cards/machine-learning/embedding/negative-sampling.md on Datumorphism</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><atom:link href="/links/cards/machine-learning/embedding/negative-sampling.md/index.xml" rel="self" type="application/rss+xml"/><item><title>Word2vec</title><link>/wiki/machine-learning/embedding/word2vec/</link><pubDate>Thu, 13 Jun 2019 00:00:00 +0000</pubDate><guid>/wiki/machine-learning/embedding/word2vec/</guid><description>Word2vec is a word embedding model that learns the probability of some words being neighbours in a sentence $p_{neighbours}(w_i, w_o)$.
Build a dataset of adjacent words. CBOW; skipgram; negative sampling; Encode the words using vectors. Build a model $f(\{\theta_i\})$ to calculate the probability of the words being neighours and improve the parameters $\{\theta_i\}$ using the dataset.</description></item></channel></rss>