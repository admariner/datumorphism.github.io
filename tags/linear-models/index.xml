<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Linear Models on Datumorphism</title><link>/tags/linear-models/</link><description>Recent content in Linear Models on Datumorphism</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><lastBuildDate>Fri, 25 May 2018 00:00:00 +0000</lastBuildDate><atom:link href="/tags/linear-models/index.xml" rel="self" type="application/rss+xml"/><item><title>Linear Methods</title><link>/wiki/machine-learning/linear/linear-methods/</link><pubDate>Fri, 25 May 2018 00:00:00 +0000</pubDate><guid>/wiki/machine-learning/linear/linear-methods/</guid><description>Solving Classification Problems with Linear Models One simple idea behind classification is to calculate the posterior probability of each class given the variables.
Suppose a dataset have features $F_\alpha$ where $\alpha = 1, 2, \cdots, K$, with corresponding class labels $G_\alpha$. The dataset that provides $N$ datapoints with each deoted as $X_i$. The posterior of the classification is $P(G = G_\alpha \vert X = X_i)$.
A naive idea is to classify the data into two classes $m$ and $n$ using the boundary of a linear model</description></item></channel></rss>