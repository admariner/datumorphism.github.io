<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Machine Learning on Datumorphism</title><link>/tags/machine-learning/</link><description>Recent content in Machine Learning on Datumorphism</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><lastBuildDate>Wed, 13 May 2020 00:00:00 +0000</lastBuildDate><atom:link href="/tags/machine-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>Receiver Operating Characteristics: ROC</title><link>/wiki/machine-learning/performance/roc/</link><pubDate>Wed, 13 May 2020 00:00:00 +0000</pubDate><guid>/wiki/machine-learning/performance/roc/</guid><description>ROC space is the two-dimensional space spanned by True Positive Rate and False Positive Rate.
ROC Space. The color boxes are indicating the confusion matrices. Green is the fraction of true positive. Orange is the fraction of false positive. Refer to Confusion Matrix for more details.
AUC: Area under Curve TPR = TP Rate FPR = FP Rate The ROC curve is defined by the relation $f(TPR, FPR)$.</description></item><item><title>Embedding</title><link>/wiki/machine-learning/embedding/overview/</link><pubDate>Sun, 13 Oct 2019 00:00:00 +0000</pubDate><guid>/wiki/machine-learning/embedding/overview/</guid><description/></item><item><title>Feature Engineering</title><link>/wiki/machine-learning/feature-engineering/overview/</link><pubDate>Mon, 17 Jun 2019 00:00:00 +0000</pubDate><guid>/wiki/machine-learning/feature-engineering/overview/</guid><description/></item><item><title>Naive Bayes</title><link>/wiki/machine-learning/bayesian/naive-bayesian/</link><pubDate>Mon, 17 Jun 2019 00:00:00 +0000</pubDate><guid>/wiki/machine-learning/bayesian/naive-bayesian/</guid><description>Naive Bayesian is a classifier using Bayes&amp;rsquo; theorem with &amp;lsquo;naive&amp;rsquo; assumptioins.
Suppose we are solving a classification problem, with features denoted as $\mathbf X$, and class results as $\mathbf Y$. We would like to train a classifier for the class results given some feature values. Bayes&amp;rsquo; theorem tells us the probability
$$ \begin{equation} P(\mathbf Y \mid \mathbf X) = \frac{ P(\mathbf X \mid \mathbf Y) P(\mathbf Y) }{ P(\mathbf X) }.</description></item><item><title>Artificial Neural Networks</title><link>/wiki/machine-learning/neural-networks/artificial-neural-networks/</link><pubDate>Mon, 19 Nov 2018 00:00:00 +0000</pubDate><guid>/wiki/machine-learning/neural-networks/artificial-neural-networks/</guid><description>Artificial neural networks works pretty well for solving some differential equations.
Universal Approximators Maxwell Stinchcombe and Halber White proved that no theoretical constraints for the feedforward networks to approximate any measurable function. In principle, one can use feedforward networks to approximate measurable functions to any accuracy.
However, the convergence slows down if we have a lot of hidden units. There is a balance between accuracy and convergence rate. More hidden units lead to slow convergence but more accuracy.</description></item><item><title>Machine Learning Overview</title><link>/wiki/machine-learning/overview/</link><pubDate>Fri, 25 May 2018 00:00:00 +0000</pubDate><guid>/wiki/machine-learning/overview/</guid><description>What is Machine Learning There are many objectives in machine learning. Two of the most applied objectives are classifications and regressions. In classifications and regression, the following four factors are relevant.
A simple framework of machine learning. The dataset $\tilde{\mathscr D}$ is first encoded by $\mathscr T$, $\mathscr D(\mathbf X, \mathbf Y) = \mathscr T(\tilde{\mathscr D})$. The dataset is feeded into the model, $\bar{\mathbf Y} = f(\mathbf X;\mathbf \theta)$.</description></item><item><title>Embedding</title><link>/cards/machine-learning/embedding/</link><pubDate>Thu, 16 Jan 2020 00:00:00 +0000</pubDate><guid>/cards/machine-learning/embedding/</guid><description/></item><item><title>Data Types and Level of Measurement in Machine Learning</title><link>/wiki/machine-learning/feature-engineering/data-types/</link><pubDate>Wed, 15 Jan 2020 00:00:00 +0000</pubDate><guid>/wiki/machine-learning/feature-engineering/data-types/</guid><description>Types of Data There are several debatable categorization methods of data.
The first widely spread theory, or level of measurement, is by S. Stevens. The theory categorizes data into four types, nominal, ordinal, interval, and ratio.
Other methods are proposed for other fields of research. For example, N. R. Chrisman proposed a different method for cartography. However, these are not generic enough for data science. They are more general than a specific field of research.</description></item><item><title>Bayesian Linear Regression</title><link>/wiki/machine-learning/bayesian/bayesian-linear-regression/</link><pubDate>Tue, 18 Jun 2019 00:00:00 +0000</pubDate><guid>/wiki/machine-learning/bayesian/bayesian-linear-regression/</guid><description>Linear Regression and Likelihood The linear estimator $y$ is
$$ \begin{equation} y^n = \beta^m X_m^{\phantom{m}n}. \label{eq-linear-model} \end{equation} $$
As usual, we have redefined our data to get rid of the intercept $\beta^0$.
In ordinary linear models, we find the error being the difference between the target $\hat y$ and the estimator $y$
$$ \epsilon = \hat y - y, $$
which is required to have a minimum absolute value.
In linear regressions, we use least squares to solve the problem.</description></item><item><title>NMF: Nonnegative Matrix Factorizatioin</title><link>/wiki/machine-learning/factorization/nmf/</link><pubDate>Thu, 13 Jun 2019 00:00:00 +0000</pubDate><guid>/wiki/machine-learning/factorization/nmf/</guid><description>Decomposition To make it easier to understand, we start with a data point $\mathbf P$ in a $k$-dimensional space spanned by $k$ basis vectors $\mathbf V^k$. Naturally, we could write down the component decomposition of the point using the basis vectors $\mathbf V^k$,
$$ \mathbf P = P_k \mathbf V^k. $$
This is immediately obvious to us since we have been dealing with rank 2 $(k, 1)$ basis vectors and we are talking about the $k$ coordinates for a point.</description></item><item><title>Word2vec</title><link>/wiki/machine-learning/embedding/word2vec/</link><pubDate>Thu, 13 Jun 2019 00:00:00 +0000</pubDate><guid>/wiki/machine-learning/embedding/word2vec/</guid><description>Word2vec is a word embedding model that learns the probability of some words being neighbours in a sentence $p_{neighbours}(w_i, w_o)$.
Build a dataset of adjacent words. CBOW; skipgram; negative sampling; Encode the words using vectors. Build a model $f(\{\theta_i\})$ to calculate the probability of the words being neighours and improve the parameters $\{\theta_i\}$ using the dataset.</description></item><item><title>Learning Theories</title><link>/cards/machine-learning/learning-theories/</link><pubDate>Thu, 16 Jan 2020 00:00:00 +0000</pubDate><guid>/cards/machine-learning/learning-theories/</guid><description/></item><item><title>Feature Engineering</title><link>/wiki/machine-learning/feature-engineering/</link><pubDate>Wed, 15 Jan 2020 00:00:00 +0000</pubDate><guid>/wiki/machine-learning/feature-engineering/</guid><description/></item><item><title>Tensor Factorization</title><link>/wiki/machine-learning/factorization/tensor-factorization/</link><pubDate>Mon, 17 Jun 2019 00:00:00 +0000</pubDate><guid>/wiki/machine-learning/factorization/tensor-factorization/</guid><description>Tensors We will be talking about tensors but we will skip the introduction to tensor for now.
In this article, we follow a commonly used convention for tensors in physics, the abstract index notation. We will denote tensors as $T^{ab\cdots}_ {\phantom{ab\cdots}cd\cdots}$, where the latin indices such as $^{a}$ are simply a placebo for the slot for this &amp;ldquo;tensor machine&amp;rdquo;. For a given basis (coordinate system), we can write down the components of this tensor $T^{\alpha\beta\cdots} _ {\phantom{\alpha\beta\cdots}\gamma\delta\cdots}$.</description></item><item><title>Boltzmann Machine</title><link>/wiki/machine-learning/neural-networks/boltzmann-machine/</link><pubDate>Sun, 27 Aug 2017 00:00:00 +0000</pubDate><guid>/wiki/machine-learning/neural-networks/boltzmann-machine/</guid><description>Boltzmann machine is much like a spin glass model in physics. In short words, Boltzmann machine is a machine that has nodes that can take values, and the nodes are connected through some weight. It is just like any other neural nets but with complications and theoretical implications.
Boltzmann Machine and Physics To obtain a good understanding of Boltzmann machine for a physicist, we begin with Ising model. We construct a system of neurons ${ s_i}$ which can take values of 1 or -1, where each pair of them $s_i$ and $s_j$ is connected by weight $J_{ij}$.</description></item><item><title>Measurement</title><link>/cards/machine-learning/measurement/</link><pubDate>Thu, 16 Jan 2020 00:00:00 +0000</pubDate><guid>/cards/machine-learning/measurement/</guid><description/></item><item><title>Bayesian Methods</title><link>/wiki/machine-learning/bayesian/</link><pubDate>Tue, 18 Jun 2019 00:00:00 +0000</pubDate><guid>/wiki/machine-learning/bayesian/</guid><description/></item><item><title>Improving Document Ranking with Dual Word Embeddings</title><link>/reading/word2vec-in-out-embedding/</link><pubDate>Sat, 05 Oct 2019 00:00:00 +0000</pubDate><guid>/reading/word2vec-in-out-embedding/</guid><description>Word2vec produces two embedding spaces, the in-embedding and out-embedding.</description></item><item><title>My Data Wiki</title><link>/projects/wiki/</link><pubDate>Sun, 07 Apr 2019 00:00:00 +0000</pubDate><guid>/projects/wiki/</guid><description>A collection of my wiki articles related to data.</description></item><item><title>My Knowledge Cards</title><link>/projects/cards/</link><pubDate>Sun, 07 Apr 2019 00:00:00 +0000</pubDate><guid>/projects/cards/</guid><description>A collection of my snippets of knowledge</description></item><item><title>My Reading Notes</title><link>/projects/reading/</link><pubDate>Sun, 07 Apr 2019 00:00:00 +0000</pubDate><guid>/projects/reading/</guid><description>A collection of my reading notes</description></item><item><title>TIL</title><link>/projects/til/</link><pubDate>Sun, 07 Apr 2019 00:00:00 +0000</pubDate><guid>/projects/til/</guid><description>Today I Learned</description></item></channel></rss>