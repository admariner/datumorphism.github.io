<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>machine learning on Datumorphism</title><link>/tags/machine-learning/</link><description>Recent content in machine learning on Datumorphism</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><lastBuildDate>Mon, 15 Feb 2021 00:00:00 +0000</lastBuildDate><atom:link href="/tags/machine-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>Receiver Operating Characteristics: ROC</title><link>/wiki/machine-learning/performance/roc/</link><pubDate>Wed, 13 May 2020 00:00:00 +0000</pubDate><guid>/wiki/machine-learning/performance/roc/</guid><description>ROC space is the two-dimensional space spanned by True Positive Rate and False Positive Rate.
ROC Space. The color boxes are indicating the confusion matrices. Green is the fraction of true positive. Orange is the fraction of false positive. Refer to Confusion Matrix for more details.
AUC: Area under Curve TPR = TP Rate FPR = FP Rate The ROC curve is defined by the relation $f(TPR, FPR)$.</description></item><item><title>Embedding</title><link>/wiki/machine-learning/embedding/overview/</link><pubDate>Sun, 13 Oct 2019 00:00:00 +0000</pubDate><guid>/wiki/machine-learning/embedding/overview/</guid><description/></item><item><title>Factorization</title><link>/wiki/machine-learning/factorization/overview/</link><pubDate>Mon, 17 Jun 2019 00:00:00 +0000</pubDate><guid>/wiki/machine-learning/factorization/overview/</guid><description/></item><item><title>Feature Engineering</title><link>/wiki/machine-learning/feature-engineering/overview/</link><pubDate>Mon, 17 Jun 2019 00:00:00 +0000</pubDate><guid>/wiki/machine-learning/feature-engineering/overview/</guid><description/></item><item><title>Naive Bayes</title><link>/wiki/machine-learning/bayesian/naive-bayes/</link><pubDate>Mon, 17 Jun 2019 00:00:00 +0000</pubDate><guid>/wiki/machine-learning/bayesian/naive-bayes/</guid><description>Naive Bayesian is a classifier using Bayes&amp;#39; Theorem Bayes&amp;rsquo; Theorem is stated as $$ P(A\mid B) = \frac{P(B \mid A) P(A)}{P(B)} $$ $P(A\mid B)$: likelihood of A given B $P(A)$: marginal probability of A There is a nice tree diagram for the Bayes&amp;rsquo; theorem on Wikipedia. Tree diagram of Bayes&amp;rsquo; theorem with &amp;lsquo;naive&amp;rsquo; assumptions.
Problems with Conditional Probability Calculation By definition, the conditional probability of event $\mathbf Y$ given features $\mathbf X$ is $$ \begin{equation} P(\mathbf Y\mid \mathbf X) = \frac{P(\mathbf Y, \mathbf X)}{ P(\mathbf X) }, \label{def-cp-y-given-x} \end{equation} $$ where</description></item><item><title>Artificial Neural Networks</title><link>/wiki/machine-learning/neural-networks/artificial-neural-networks/</link><pubDate>Mon, 19 Nov 2018 00:00:00 +0000</pubDate><guid>/wiki/machine-learning/neural-networks/artificial-neural-networks/</guid><description>Artificial neural networks works pretty well for solving some differential equations.
Universal Approximators Maxwell Stinchcombe and Halber White proved that no theoretical constraints for the feedforward networks to approximate any measurable function. In principle, one can use feedforward networks to approximate measurable functions to any accuracy.
However, the convergence slows down if we have a lot of hidden units. There is a balance between accuracy and convergence rate. More hidden units lead to slow convergence but more accuracy.</description></item><item><title>Machine Learning Overview</title><link>/wiki/machine-learning/overview/</link><pubDate>Fri, 25 May 2018 00:00:00 +0000</pubDate><guid>/wiki/machine-learning/overview/</guid><description>What is Machine Learning There are many objectives in machine learning. Two of the most applied objectives are classifications and regressions. In classifications and regression, the following four factors are relevant.
A simple framework of machine learning. The dataset $\tilde{\mathscr D}$ is first encoded by $\mathscr T$, $\mathscr D(\mathbf X, \mathbf Y) = \mathscr T(\tilde{\mathscr D})$. The dataset is feeded into the model, $\bar{\mathbf Y} = f(\mathbf X;\mathbf \theta)$.</description></item><item><title>Embedding</title><link>/cards/machine-learning/embedding/</link><pubDate>Thu, 16 Jan 2020 00:00:00 +0000</pubDate><guid>/cards/machine-learning/embedding/</guid><description/></item><item><title>Data Types and Level of Measurement in Machine Learning</title><link>/wiki/machine-learning/feature-engineering/data-types/</link><pubDate>Wed, 15 Jan 2020 00:00:00 +0000</pubDate><guid>/wiki/machine-learning/feature-engineering/data-types/</guid><description>Types of Data There are several debatable categorization methods of data.
The first widely spread theory, or level of measurement, is by S. Stevens. The theory categorizes data into four types, nominal, ordinal, interval, and ratio.
Other methods are proposed for other fields of research. For example, N. R. Chrisman proposed a different method for cartography. However, these are not generic enough for data science. They are more general than a specific field of research.</description></item><item><title>Bayesian Linear Regression</title><link>/wiki/machine-learning/bayesian/bayesian-linear-regression/</link><pubDate>Tue, 18 Jun 2019 00:00:00 +0000</pubDate><guid>/wiki/machine-learning/bayesian/bayesian-linear-regression/</guid><description>Linear Regression and Likelihood The linear estimator $y$ is
$$ \begin{equation} y^n = \beta^m X_m^{\phantom{m}n}. \label{eq-linear-model} \end{equation} $$
As usual, we have redefined our data to get rid of the intercept $\beta^0$.
In ordinary linear models, we find the error being the difference between the target $\hat y$ and the estimator $y$
$$ \epsilon = \hat y - y, $$
which is required to have a minimum absolute value.
In linear regressions, we use least squares to solve the problem.</description></item><item><title>NMF: Nonnegative Matrix Factorizatioin</title><link>/wiki/machine-learning/factorization/nmf/</link><pubDate>Thu, 13 Jun 2019 00:00:00 +0000</pubDate><guid>/wiki/machine-learning/factorization/nmf/</guid><description>Decomposition To make it easier to understand, we start with a data point $\mathbf P$ in a $k$-dimensional space spanned by $k$ basis vectors $\mathbf V^k$. Naturally, we could write down the component decomposition of the point using the basis vectors $\mathbf V^k$,
$$ \mathbf P = P_k \mathbf V^k. $$
This is immediately obvious to us since we have been dealing with rank 2 $(k, 1)$ basis vectors and we are talking about the $k$ coordinates for a point.</description></item><item><title>Word2vec</title><link>/wiki/machine-learning/embedding/word2vec/</link><pubDate>Thu, 13 Jun 2019 00:00:00 +0000</pubDate><guid>/wiki/machine-learning/embedding/word2vec/</guid><description>Word2vec is a word embedding model that learns the probability of some words being neighbours in a sentence $p_{neighbours}(w_i, w_o)$.
Build a dataset of adjacent words. CBOW; skipgram; negative sampling; Encode the words using vectors. Build a model $f(\{\theta_i\})$ to calculate the probability of the words being neighours and improve the parameters $\{\theta_i\}$ using the dataset.</description></item><item><title>A Physicist's Crash Course on Artificial Neural Network</title><link>/wiki/machine-learning/neural-networks/physicists-crash-course-neural-network/</link><pubDate>Sat, 02 May 2015 00:00:00 +0000</pubDate><guid>/wiki/machine-learning/neural-networks/physicists-crash-course-neural-network/</guid><description>What is a Neuron What a neuron does is to response when a stimulation is given. This response could be strong or weak or even null. If I would draw a figure, of this behavior, it looks like this.
Neuron response Using simple single neuron responses, we could compose complicated responses. To achieve that, we study the transformations of the response first.
transformations Artificial Neural Network A simple network is a collection of neurons that response to stimulations, which could be the responses of other neurons.</description></item><item><title>Learning Theories</title><link>/cards/machine-learning/learning-theories/</link><pubDate>Thu, 18 Feb 2021 00:00:00 +0000</pubDate><guid>/cards/machine-learning/learning-theories/</guid><description/></item><item><title>Feature Engineering</title><link>/wiki/machine-learning/feature-engineering/</link><pubDate>Wed, 15 Jan 2020 00:00:00 +0000</pubDate><guid>/wiki/machine-learning/feature-engineering/</guid><description/></item><item><title>Tensor Factorization</title><link>/wiki/machine-learning/factorization/tensor-factorization/</link><pubDate>Mon, 17 Jun 2019 00:00:00 +0000</pubDate><guid>/wiki/machine-learning/factorization/tensor-factorization/</guid><description>Tensors We will be talking about tensors but we will skip the introduction to tensor for now.
In this article, we follow a commonly used convention for tensors in physics, the abstract index notation. We will denote tensors as $T^{ab\cdots}_ {\phantom{ab\cdots}cd\cdots}$, where the latin indices such as $^{a}$ are simply a placebo for the slot for this &amp;ldquo;tensor machine&amp;rdquo;. For a given basis (coordinate system), we can write down the components of this tensor $T^{\alpha\beta\cdots} _ {\phantom{\alpha\beta\cdots}\gamma\delta\cdots}$.</description></item><item><title>Boltzmann Machine</title><link>/wiki/machine-learning/neural-networks/boltzmann-machine/</link><pubDate>Sun, 27 Aug 2017 00:00:00 +0000</pubDate><guid>/wiki/machine-learning/neural-networks/boltzmann-machine/</guid><description>Boltzmann machine is much like a spin glass model in physics. In short words, Boltzmann machine is a machine that has nodes that can take values, and the nodes are connected through some weight. It is just like any other neural nets but with complications and theoretical implications.
Boltzmann Machine and Physics To obtain a good understanding of Boltzmann machine for a physicist, we begin with Ising model. We construct a system of neurons ${ s_i}$ which can take values of 1 or -1, where each pair of them $s_i$ and $s_j$ is connected by weight $J_{ij}$.</description></item><item><title>Deep Autoregressive Network</title><link>/wiki/machine-learning/neural-networks/deep-autoregressive-networks/</link><pubDate>Mon, 15 Feb 2021 00:00:00 +0000</pubDate><guid>/wiki/machine-learning/neural-networks/deep-autoregressive-networks/</guid><description>There are two levels of autoregressiveness in the DARN network:
Inlayer autoregressive connections of the nodes, Intralayer autoregressive connections of nodes. The network is trained on MDL loss.</description></item><item><title>Measurement</title><link>/cards/machine-learning/measurement/</link><pubDate>Sat, 06 Feb 2021 00:00:00 +0000</pubDate><guid>/cards/machine-learning/measurement/</guid><description/></item><item><title>Some ML Workflow Frameworks</title><link>/wiki/tools/ml-flow-frameworks/</link><pubDate>Wed, 13 Jan 2021 00:00:00 +0000</pubDate><guid>/wiki/tools/ml-flow-frameworks/</guid><description>Metaflow Docs
A framework for jupyter notebook data scientists.
Work locally on notebooks. Python environment management using conda. Work in the cloud with Sagemaker. Tasks Methods Comments Code Scripts/Jupyter Notebook Datastore local + S3 metaflow.S3 Compute local + AWS Batch Metadata metaflow service Metadata specifies flow executions: Flows, Runs, Steps, Tasks, and Artifacts. Scheduling AWS Step Functions Deployment AWS Demo from metaflow import FlowSpec, step class BranchFlow(FlowSpec): @step def start(self): self.</description></item><item><title>Bayesian Methods</title><link>/wiki/machine-learning/bayesian/</link><pubDate>Wed, 27 Jan 2021 00:00:00 +0000</pubDate><guid>/wiki/machine-learning/bayesian/</guid><description/></item><item><title>Machine as a Hologram</title><link>/projects/hologram/</link><pubDate>Sun, 31 Jan 2021 00:00:00 +0000</pubDate><guid>/projects/hologram/</guid><description>Tutorials on machine learning and data science productivity articles</description></item><item><title>Normalizing Flows: An Introduction and Review of Current Methods</title><link>/reading/normalizing-flow-introduction-1908.09257/</link><pubDate>Sun, 17 Jan 2021 00:00:00 +0000</pubDate><guid>/reading/normalizing-flow-introduction-1908.09257/</guid><description>To generate complicated distributions step by step from a simple and interpretable distribution.</description></item><item><title>Improving Document Ranking with Dual Word Embeddings</title><link>/reading/word2vec-in-out-embedding/</link><pubDate>Sat, 05 Oct 2019 00:00:00 +0000</pubDate><guid>/reading/word2vec-in-out-embedding/</guid><description>Word2vec produces two embedding spaces, the in-embedding and out-embedding.</description></item><item><title>My Data Wiki</title><link>/projects/wiki/</link><pubDate>Sun, 07 Apr 2019 00:00:00 +0000</pubDate><guid>/projects/wiki/</guid><description>A collection of my wiki articles related to data.</description></item><item><title>My Knowledge Cards</title><link>/projects/cards/</link><pubDate>Sun, 07 Apr 2019 00:00:00 +0000</pubDate><guid>/projects/cards/</guid><description>A collection of my snippets of knowledge</description></item><item><title>My Reading Notes</title><link>/projects/reading/</link><pubDate>Sun, 07 Apr 2019 00:00:00 +0000</pubDate><guid>/projects/reading/</guid><description>A collection of my reading notes</description></item><item><title>TIL</title><link>/projects/til/</link><pubDate>Sun, 07 Apr 2019 00:00:00 +0000</pubDate><guid>/projects/til/</guid><description>Today I Learned</description></item><item><title>Workflows</title><link>/awesome/workflows/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/awesome/workflows/</guid><description>The scope of exploratory data analysis is not universally defined. Some of the contents discussed here may have crossed the line. The whole modeling process is never decoupled anyway. Data wrangling is mostly guided by the exploratory data analysis (EDA). In other words, the data cleaning process should be mostly guided by questions from business and stakeholder or out of curiosity.
There are three key components in EDA.</description></item></channel></rss>