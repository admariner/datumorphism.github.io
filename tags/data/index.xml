<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Data on Datumorphism</title><link>/tags/data/</link><description>Recent content in Data on Datumorphism</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><lastBuildDate>Fri, 08 Jan 2021 00:00:00 +0000</lastBuildDate><atom:link href="/tags/data/index.xml" rel="self" type="application/rss+xml"/><item><title>Sugar</title><link>/wiki/sugar/</link><pubDate>Wed, 20 Jun 2018 15:58:49 -0400</pubDate><guid>/wiki/sugar/</guid><description/></item><item><title>Data Types</title><link>/cards/machine-learning/datatypes/data-types/</link><pubDate>Thu, 16 Jan 2020 00:00:00 +0000</pubDate><guid>/cards/machine-learning/datatypes/data-types/</guid><description/></item><item><title>Gini Impurity</title><link>/cards/machine-learning/measurement/gini-impurity/</link><pubDate>Thu, 16 Jan 2020 00:00:00 +0000</pubDate><guid>/cards/machine-learning/measurement/gini-impurity/</guid><description>The code used in this article can be found in this repo. Suppose we have a dataset $\{0,1\}^{10}$, which has 10 records and 2 possible classes of objects $\{0,1\}$ in each record.
The first example we investigate is a pure 0 dataset.
object 0 0 0 0 0 0 0 0 0 0 0 0 For such an all-0 dataset, we would like to define its impurity as 0.</description></item><item><title>Information Gain</title><link>/cards/machine-learning/measurement/information-gain/</link><pubDate>Thu, 16 Jan 2020 00:00:00 +0000</pubDate><guid>/cards/machine-learning/measurement/information-gain/</guid><description>Information gain is a frequently used metric in calculating the gain during a split in tree-based methods.
First o all, the entropy of a dataset if defined as
$$ S = - sum_i p_i \log p_i - sum_i (1-p_i)\log p_i, $$
where $p_i$ is the probability of a class.
The information gain is the difference between the entropy.
For example, in a decision tree algorithm, we would split a node. Before splitting, we assign a label $m$ to the node,</description></item><item><title>PAC: Probably Approximately Correct</title><link>/cards/machine-learning/learning-theories/pac/</link><pubDate>Thu, 16 Jan 2020 00:00:00 +0000</pubDate><guid>/cards/machine-learning/learning-theories/pac/</guid><description/></item></channel></rss>