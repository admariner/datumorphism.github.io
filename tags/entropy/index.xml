<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Entropy on Datumorphism</title><link>https://datumorphism.leima.is/tags/entropy/</link><description>Recent content in Entropy on Datumorphism</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><lastBuildDate>Fri, 11 Jun 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://datumorphism.leima.is/tags/entropy/index.xml" rel="self" type="application/rss+xml"/><item><title>MaxEnt Model</title><link>https://datumorphism.leima.is/wiki/machine-learning/energy-based-model/maxent-energy-based-model/</link><pubDate>Mon, 31 May 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/energy-based-model/maxent-energy-based-model/</guid><description>The Maximum Entropy model, aka MaxEnt model, is a fascinating generative model as it is based on a very intuitive idea from statistical physics - the Principle of Maximum Entropy.
The Idea The essence of the MaxEnt model is that the underlying probability distribution $p(x)$ of the random variables $x$ should
gives the whole system the largest uncertainty, while producing reasonable observables. Uncertainty The uncertainty of the whole system is described by the Shannon entropy based on the probability distributions $p(x)$,</description></item><item><title>Restricted Boltzmann Machine</title><link>https://datumorphism.leima.is/wiki/machine-learning/energy-based-model/restricted-boltzmann-machine/</link><pubDate>Fri, 11 Jun 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/energy-based-model/restricted-boltzmann-machine/</guid><description>Latent variables introduce extra correlations between the nodes in a network. Introducing hidden units can also help us remove the direct connection between some nodes in a Boltzmann machine and create a restricted Boltzmann machine. A restricted Boltzmann machine requires less computation while having some expressing power.
Given Ising like interactions between the nodes, flipping node V1 is likely to also flip node V2 as they are connected through hidden unit H1.</description></item><item><title>Coding Theory Concepts</title><link>https://datumorphism.leima.is/cards/information/coding-theory-concepts/</link><pubDate>Wed, 17 Feb 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/information/coding-theory-concepts/</guid><description>The code function produces code words. The expected length of the code word is limited by the entropy from the source probability $p$.
The Shannon information content, aka self-information, is described by
$$ - \log_2 p(x=a), $$
for the case that $x=a$.
The Shannon entropy is the expected information content for the whole sequence with probability distribution $p(x)$,
$$ \mathcal H = - \sum_x p(x\in X) \log_2 p(x). $$
The Shannon source coding theorem says that for $N$ samples from the source, we can roughly compress it into $N\mathcal H$.</description></item></channel></rss>