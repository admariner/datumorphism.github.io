<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Model Selection on Datumorphism</title><link>/tags/model-selection/</link><description>Recent content in Model Selection on Datumorphism</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><lastBuildDate>Sun, 08 Nov 2020 00:00:00 +0000</lastBuildDate><atom:link href="/tags/model-selection/index.xml" rel="self" type="application/rss+xml"/><item><title>Akaike Information Criterion</title><link>/cards/statistics/aic/</link><pubDate>Sun, 08 Nov 2020 00:00:00 +0000</pubDate><guid>/cards/statistics/aic/</guid><description>Suppose we have a model that describes the data generation process behind a dataset. The distribution by the model is denoted as $\hat f$. The actual data generation process is described by a distribution $f$.
We ask the question:
How good is the approximation using $\hat f$?
To be more precise, how much information is lost if we use our model dist $\hat f$ to substitute the actual data generation distribution $f$?</description></item><item><title>Bayes Factors</title><link>/cards/statistics/bayes-factors/</link><pubDate>Sun, 08 Nov 2020 00:00:00 +0000</pubDate><guid>/cards/statistics/bayes-factors/</guid><description>$$ \frac{p(\mathscr M_1|y)}{ p(\mathscr M_2|y) } = \frac{p(\mathscr M_1)}{ p(\mathscr M_2) }\frac{p(y|\mathscr M_1)}{ p(y|\mathscr M_2) } $$ Bayes factor
$$ \mathrm{BF_{12}} = \frac{m(y|\mathscr M_1)}{m(y|\mathscr M_2)} $$ $\mathrm{BF_{12}}$: how many time more likely is model $\mathscr M_1$ than $\mathscr M_2$.</description></item><item><title>Bayesian Information Criterion</title><link>/cards/statistics/bic/</link><pubDate>Sun, 08 Nov 2020 00:00:00 +0000</pubDate><guid>/cards/statistics/bic/</guid><description>BIC is Bayesian information criterion, it replaced the $+2k$ term in AIC Suppose we have a model that describes the data generation process behind a dataset. The distribution by the model is denoted as $\hat f$. The actual data generation process is described by a distribution $f$. We ask the question: How good is the approximation using $\hat f$? To be more precise, how much information is lost if we use our model dist $\hat f$ to substitute the actual data generation distribution $f$?</description></item><item><title>Fisher Information Approximation</title><link>/cards/statistics/fia/</link><pubDate>Sun, 08 Nov 2020 00:00:00 +0000</pubDate><guid>/cards/statistics/fia/</guid><description>#FIA is a method to describe the [[minimum-description-length|minimum description length ( #MDL )]] of models,
$$ \mathrm{FIA} = -\ln p(y | \hat\theta) + \frac{k}{2} \ln \frac{n}{2\pi} + \ln \int_\Theta \sqrt{ \operatorname{det}[I(\theta)] d\theta } $$
$I(\theta)$: Fisher information matrix of sample size 1. $$I_{i,j}(\theta) = E\left( \frac{\partial \ln p(y| \theta)}{\partial \theta_i}\frac{ \partial \ln p (y | \theta) }{ \partial \theta_j } \right)$$.</description></item><item><title>Kolmogorov Complexity</title><link>/cards/statistics/kolmogorov-complexity/</link><pubDate>Sun, 08 Nov 2020 00:00:00 +0000</pubDate><guid>/cards/statistics/kolmogorov-complexity/</guid><description>Description:
$\Sigma=\{0,1\}$, a map $f:\Sigma^* \to\Sigma^*$. To describe a string of 0 and 1 $\sigma$, the description is a map so that $f(\tau)=\sigma$.
Kolmogorov complexity $C_f$
$$ C_f(x) = \begin{cases} min\{ \vert p \vert : f(p) = x &amp; \text{if x} \\ \infty &amp; \text{otherwise} \} \end{cases} $$ $f$ can be a universal turing machine.</description></item></channel></rss>