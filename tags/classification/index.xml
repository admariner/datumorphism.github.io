<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Classification on Datumorphism</title><link>/tags/classification/</link><description>Recent content in Classification on Datumorphism</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><lastBuildDate>Wed, 25 Dec 2019 00:00:00 +0000</lastBuildDate><atom:link href="/tags/classification/index.xml" rel="self" type="application/rss+xml"/><item><title>Tree-based Learning</title><link>/wiki/machine-learning/tree-based/overview/</link><pubDate>Wed, 25 Dec 2019 00:00:00 +0000</pubDate><guid>/wiki/machine-learning/tree-based/overview/</guid><description>Decision tree is an easy-to-interpret method in supervised learning. Though simple, it is being used in some widely used algorithms such as random forest method.</description></item><item><title>Naive Bayes</title><link>/wiki/machine-learning/bayesian/naive-bayesian/</link><pubDate>Mon, 17 Jun 2019 00:00:00 +0000</pubDate><guid>/wiki/machine-learning/bayesian/naive-bayesian/</guid><description>Naive Bayesian is a classifier using Bayes&amp;#39; Theorem Bayes&amp;rsquo; Theorem is stated as $$ P(A\mid B) = \frac{P(B \mid A) P(A)}{P(B)} $$ $P(A\mid B)$: likelihood of A given B $P(A)$: marginal probability of A There is a nice tree diagram for the Bayes&amp;rsquo; theorem on Wikipedia. Tree diagram of Bayes&amp;rsquo; theorem with &amp;lsquo;naive&amp;rsquo; assumptions.
Problems with Conditional Probability Calculation By definition, the conditional probability of event $\mathbf Y$ given features $\mathbf X$ is $$ \begin{equation} P(\mathbf Y\mid \mathbf X) = \frac{P(\mathbf Y, \mathbf X)}{ P(\mathbf X) }, \label{def-cp-y-given-x} \end{equation} $$ where</description></item><item><title>Confusion Matrix (Contingency Table)</title><link>/wiki/machine-learning/basics/confusion-matrix/</link><pubDate>Fri, 31 May 2019 00:00:00 +0000</pubDate><guid>/wiki/machine-learning/basics/confusion-matrix/</guid><description>Confusion Matrix It is much easier to understand the confusion matrix if we use a binary classification problem as an example. For example, we have a bunch of cat photos and the user labeled &amp;ldquo;cute or not&amp;rdquo; data. Now we are using the labeled data to train a cute-or-not binary classifier.
Then we apply the classifier on the test dataset and we would only find four different kinds of results.</description></item><item><title>Linear Methods</title><link>/wiki/machine-learning/linear/linear-methods/</link><pubDate>Fri, 25 May 2018 00:00:00 +0000</pubDate><guid>/wiki/machine-learning/linear/linear-methods/</guid><description>Solving Classification Problems with Linear Models One simple idea behind classification is to calculate the posterior probability of each class given the variables.
Suppose a dataset have features $F_\alpha$ where $\alpha = 1, 2, \cdots, K$, with corresponding class labels $G_\alpha$. The dataset that provides $N$ datapoints with each deoted as $X_i$. The posterior of the classification is $P(G = G_\alpha \vert X = X_i)$.
A naive idea is to classify the data into two classes $m$ and $n$ using the boundary of a linear model</description></item></channel></rss>