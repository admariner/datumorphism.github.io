<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Learning Theories on Datumorphism</title><link>/cards/machine-learning/learning-theories/</link><description>Recent content in Learning Theories on Datumorphism</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><lastBuildDate>Thu, 18 Feb 2021 00:00:00 +0000</lastBuildDate><atom:link href="/cards/machine-learning/learning-theories/index.xml" rel="self" type="application/rss+xml"/><item><title>ERM: Empirical Risk Minimization</title><link>/cards/machine-learning/learning-theories/empirical-risk-minimization/</link><pubDate>Thu, 18 Feb 2021 00:00:00 +0000</pubDate><guid>/cards/machine-learning/learning-theories/empirical-risk-minimization/</guid><description>A negative log likelihood (NLL) for a model $\theta$ of dataset $\mathcal D$
$$ NLL(\theta) = -\log p(\mathcal D\mid\theta) = -\sum_n \log (y_n \mid x_n, \theta). $$ An empirical risk loss function $\mathcal L$ is
$$ \mathcal L(\theta) = \frac{1}{N} \sum_n \mathscr l(y_n, \theta; x_n), $$ where $\mathscr l$ is a loss. For example, one could design a stepwise loss in classification
$$ \mathscr l = \begin{cases} 0, \qquad \text{if prediction matches data} \\ 1 \qquad \text{if prediction doesn't match data} \end{cases} $$ Another possibility is surrogate loss which is continuous.</description></item><item><title>PAC: Probably Approximately Correct</title><link>/cards/machine-learning/learning-theories/pac/</link><pubDate>Thu, 16 Jan 2020 00:00:00 +0000</pubDate><guid>/cards/machine-learning/learning-theories/pac/</guid><description/></item></channel></rss>